{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multiple_Summarizer_Tests.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOn09tUhYO3Jq0BI0IczNzC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skimotv/SkimoTextSummarizer/blob/master/Multiple_Summarizer_Tests.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIayaWsuzFrj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "outputId": "0025dfcb-87e4-4cdd-ab84-bd99a223f97f"
      },
      "source": [
        "!pip install -U git+https://github.com/huggingface/transformers.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers.git\n",
            "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-cmce0ljz\n",
            "  Running command git clone -q https://github.com/huggingface/transformers.git /tmp/pip-req-build-cmce0ljz\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (0.8.1rc2)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (20.4)\n",
            "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (3.0.12)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (0.1.91)\n",
            "Requirement already satisfied, skipping upgrade: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (0.0.43)\n",
            "Requirement already satisfied, skipping upgrade: dataclasses in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (0.7)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.0.2) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.0.2) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.2) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.2) (0.16.0)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-3.0.2-cp36-none-any.whl size=876677 sha256=af0f3546f38b8321369f96429a5de79b80506f4a97661f62652aa4e4c36b649f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-xal_0lss/wheels/33/eb/3b/4bf5dd835e865e472d4fc0754f35ac0edb08fe852e8f21655f\n",
            "Successfully built transformers\n",
            "Installing collected packages: transformers\n",
            "  Found existing installation: transformers 3.0.2\n",
            "    Uninstalling transformers-3.0.2:\n",
            "      Successfully uninstalled transformers-3.0.2\n",
            "Successfully installed transformers-3.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymHPRLALImqy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "0ab43314-5f26-43c1-81e4-24d77ec6bbd2"
      },
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelWithLMHead"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PyTorch version 1.6.0+cu101 available.\n",
            "TensorFlow version 2.3.0 available.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_cq4JuCy8Ol",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "63d7eed0-c3b2-4615-f1d8-3dd88b6e1a1c"
      },
      "source": [
        "# Use a model from https://huggingface.co/models?filter=summarization&sort=modified\n",
        "models = [\"sshleifer/distilbart-cnn-12-3\", \"sshleifer/distilbart-cnn-12-6\",\n",
        "          \"sshleifer/distilbart-cnn-6-6\", \"sshleifer/distilbart-xsum-1-1\",\n",
        "          \"sshleifer/distilbart-xsum-12-1\", \"sshleifer/distilbart-xsum-12-3\", \n",
        "          \"sshleifer/distilbart-xsum-12-6\", \"sshleifer/distilbart-xsum-6-6\", \n",
        "          \"sshleifer/distilbart-xsum-9-6\", \"google/pegasus-billsum\"\n",
        "          ]\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(models[9])\n",
        "model = AutoModelWithLMHead.from_pretrained(models[9])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/google/pegasus-billsum/config.json from cache at /root/.cache/torch/transformers/31922cdb6e3bdcc3b13bac9c4d3b8bdb142cf7dad7fb714a7e1a48efb8f44c32.a29ae7c0ab72e787add16a85092d2219c56226f2a452ee8857c1fcace10baaf4\n",
            "Model config PegasusConfig {\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": true,\n",
            "  \"architectures\": [\n",
            "    \"PegasusForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 16,\n",
            "  \"dropout\": 0.1,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 16,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"extra_pos_embeddings\": 1,\n",
            "  \"force_bos_token_to_be_generated\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"length_penalty\": 0.6,\n",
            "  \"max_length\": 256,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"pegasus\",\n",
            "  \"normalize_before\": true,\n",
            "  \"normalize_embedding\": false,\n",
            "  \"num_beams\": 8,\n",
            "  \"num_hidden_layers\": 16,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"scale_embedding\": true,\n",
            "  \"static_position_embeddings\": true,\n",
            "  \"vocab_size\": 96103\n",
            "}\n",
            "\n",
            "Model name 'google/pegasus-billsum' not found in model shortcut name list (google/reformer-crime-and-punishment). Assuming 'google/pegasus-billsum' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "https://s3.amazonaws.com/models.huggingface.co/bert/google/pegasus-billsum/spiece.model not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpopn0z1zf\n",
            "storing https://s3.amazonaws.com/models.huggingface.co/bert/google/pegasus-billsum/spiece.model in cache at /root/.cache/torch/transformers/0de312684f23595e15cad72e1c0a7704998a76a9c13dde2deb4395114be457be.efce77b8dcd2c57b109b0d10170fcdcd53f23c21286974d4f66706536758ab6e\n",
            "creating metadata file for /root/.cache/torch/transformers/0de312684f23595e15cad72e1c0a7704998a76a9c13dde2deb4395114be457be.efce77b8dcd2c57b109b0d10170fcdcd53f23c21286974d4f66706536758ab6e\n",
            "https://s3.amazonaws.com/models.huggingface.co/bert/google/pegasus-billsum/special_tokens_map.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp1hihwbgv\n",
            "storing https://s3.amazonaws.com/models.huggingface.co/bert/google/pegasus-billsum/special_tokens_map.json in cache at /root/.cache/torch/transformers/a78c8b4a12d69e39d89d5344005451821caf678e9149f67738e18e8b4894c48c.d142dfa55f201f5033fe9ee40eb8fe1ca965dcb0f38b175386020492986d507f\n",
            "creating metadata file for /root/.cache/torch/transformers/a78c8b4a12d69e39d89d5344005451821caf678e9149f67738e18e8b4894c48c.d142dfa55f201f5033fe9ee40eb8fe1ca965dcb0f38b175386020492986d507f\n",
            "https://s3.amazonaws.com/models.huggingface.co/bert/google/pegasus-billsum/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpctrn3zxi\n",
            "storing https://s3.amazonaws.com/models.huggingface.co/bert/google/pegasus-billsum/tokenizer_config.json in cache at /root/.cache/torch/transformers/d23042c9f62ff9112c0952b3254a91ef462e8997fbe29e3e6e30a08b4af9f80e.d998fec0f40be5bce279bd4d1ee3aec97da46fd28646ea657c3acf2a4563840a\n",
            "creating metadata file for /root/.cache/torch/transformers/d23042c9f62ff9112c0952b3254a91ef462e8997fbe29e3e6e30a08b4af9f80e.d998fec0f40be5bce279bd4d1ee3aec97da46fd28646ea657c3acf2a4563840a\n",
            "loading file https://s3.amazonaws.com/models.huggingface.co/bert/google/pegasus-billsum/spiece.model from cache at /root/.cache/torch/transformers/0de312684f23595e15cad72e1c0a7704998a76a9c13dde2deb4395114be457be.efce77b8dcd2c57b109b0d10170fcdcd53f23c21286974d4f66706536758ab6e\n",
            "loading file https://s3.amazonaws.com/models.huggingface.co/bert/google/pegasus-billsum/added_tokens.json from cache at None\n",
            "loading file https://s3.amazonaws.com/models.huggingface.co/bert/google/pegasus-billsum/special_tokens_map.json from cache at /root/.cache/torch/transformers/a78c8b4a12d69e39d89d5344005451821caf678e9149f67738e18e8b4894c48c.d142dfa55f201f5033fe9ee40eb8fe1ca965dcb0f38b175386020492986d507f\n",
            "loading file https://s3.amazonaws.com/models.huggingface.co/bert/google/pegasus-billsum/tokenizer_config.json from cache at /root/.cache/torch/transformers/d23042c9f62ff9112c0952b3254a91ef462e8997fbe29e3e6e30a08b4af9f80e.d998fec0f40be5bce279bd4d1ee3aec97da46fd28646ea657c3acf2a4563840a\n",
            "loading file https://s3.amazonaws.com/models.huggingface.co/bert/google/pegasus-billsum/tokenizer.json from cache at None\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/modeling_auto.py:821: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n",
            "loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/google/pegasus-billsum/config.json from cache at /root/.cache/torch/transformers/31922cdb6e3bdcc3b13bac9c4d3b8bdb142cf7dad7fb714a7e1a48efb8f44c32.a29ae7c0ab72e787add16a85092d2219c56226f2a452ee8857c1fcace10baaf4\n",
            "Model config PegasusConfig {\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": true,\n",
            "  \"architectures\": [\n",
            "    \"PegasusForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 16,\n",
            "  \"dropout\": 0.1,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 16,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"extra_pos_embeddings\": 1,\n",
            "  \"force_bos_token_to_be_generated\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"length_penalty\": 0.6,\n",
            "  \"max_length\": 256,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"pegasus\",\n",
            "  \"normalize_before\": true,\n",
            "  \"normalize_embedding\": false,\n",
            "  \"num_beams\": 8,\n",
            "  \"num_hidden_layers\": 16,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"scale_embedding\": true,\n",
            "  \"static_position_embeddings\": true,\n",
            "  \"vocab_size\": 96103\n",
            "}\n",
            "\n",
            "https://cdn.huggingface.co/google/pegasus-billsum/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpixs82yy3\n",
            "storing https://cdn.huggingface.co/google/pegasus-billsum/pytorch_model.bin in cache at /root/.cache/torch/transformers/bbb86d1a5c5300b7704fd72ca450df7ecea975bf282a229aaeb18b6ef272c9e1.563d89addb23457143e5ec68bc9aa352d89e6e8128659f8863ac20eb763d924e\n",
            "creating metadata file for /root/.cache/torch/transformers/bbb86d1a5c5300b7704fd72ca450df7ecea975bf282a229aaeb18b6ef272c9e1.563d89addb23457143e5ec68bc9aa352d89e6e8128659f8863ac20eb763d924e\n",
            "loading weights file https://cdn.huggingface.co/google/pegasus-billsum/pytorch_model.bin from cache at /root/.cache/torch/transformers/bbb86d1a5c5300b7704fd72ca450df7ecea975bf282a229aaeb18b6ef272c9e1.563d89addb23457143e5ec68bc9aa352d89e6e8128659f8863ac20eb763d924e\n",
            "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
            "\n",
            "Some weights of BartForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-billsum and are newly initialized: ['model.encoder.embed_positions.weight', 'model.decoder.embed_positions.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "886SRB7axdVJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bb00506b-4d13-4de5-8ddf-930d77aed04a"
      },
      "source": [
        "text = \"Four overnight camps in Maine successfully identified and isolated three Covid-19 positive people with no symptoms, preventing transmission to more than 1,000 other campers and staff this summer, says a new report published by the Centers for Disease Control and Prevention.For many kids, summer camp looked and felt a little different this year. There were daily temperatures checks, more time spent outside and plenty of face masks. Dr. Laura Blaisdell of the Maine Medical Center Research Institute and colleagues said the extra effort paid off.They detailed where these camps went right in a report examining 642 children and 380 staff members who attended the four camps in Maine for well over a month between June and August.  Camp attendees traveled from across the United States and six international locations: Bermuda, Canada, Mexico, South Africa, Spain and the United Kingdom. They quarantined for up to 14 days before arriving at camp and three of the sites asked campers to submit Covid-19 test results before attending. This was an important step in preventing introduction of the virus in a setting with many young adults who could be asymptomatic or presymptomatic, Blaisdell and colleagues wrote in the CDC's weekly report.Camp attendees were separated into groups when they first arrived and had to wear face coverings when interacting with people outside of their groups. The camps kept surfaces clean and groups physically distant. They staggered bathroom use and dining times. They also screened campers daily for fever and coronavirus symptoms. Most attendees were tested again for Covid-19 a few days after arriving at camp. That's when a symptomless camper and two staff members tested positive, according to the report. They were rapidly isolated until they recovered, and their contacts were quarantined for 14 days.  (CNN)Four overnight camps in Maine successfully identified and isolated three Covid-19 positive people with no symptoms, preventing transmission to more than 1,000 other campers and staff this summer, says a new report published by the Centers for Disease Control and Prevention.For many kids, summer camp looked and felt a little different this year. There were daily temperatures checks, more time spent outside and plenty of face masks. Dr. Laura Blaisdell of the Maine Medical Center Research Institute and colleagues said the extra effort paid off.They detailed where these camps went right in a report examining 642 children and 380 staff members who attended the four camps in Maine for well over a month between June and August.A Georgia sleepaway camp&#39;s coronavirus outbreak is a warning for what could happen when schools reopen, CDC saysA Georgia sleepaway camp's coronavirus outbreak is a warning for what could happen when schools reopen, CDC saysCamp attendees traveled from across the United States and six international locations: Bermuda, Canada, Mexico, South Africa, Spain and the United Kingdom. They quarantined for up to 14 days before arriving at camp and three of the sites asked campers to submit Covid-19 test results before attending.Content by CNN UnderscoredHow to sell your old tech before it loses its value.CNN Underscored partnered with Decluttr to create this content. When you make a purchase, CNN receives revenue.This was an important step in preventing introduction of the virus in a setting with many young adults who could be asymptomatic or presymptomatic, Blaisdell and colleagues wrote in the CDC's weekly report.Camp attendees were separated into groups when they first arrived and had to wear face coverings when interacting with people outside of their groups. The camps kept surfaces clean and groups physically distant. They staggered bathroom use and dining times. They also screened campers daily for fever and coronavirus symptoms.Covid-19 child cases in the US have increased by 21% since early August, new data showsCovid-19 child cases in the US have increased by 21% since early August, new data showsMost attendees were tested again for Covid-19 a few days after arriving at camp. That's when a symptomless camper and two staff members tested positive, according to the report. They were rapidly isolated until they recovered, and their contacts were quarantined for 14 days.None of the contacts tested positive for Covid-19, according to the CDC report.The report noted that it wasn't one particular precaution that helped prevent the spread of coronavirus in these camps, but rather a multilayered strategy that was carefully executed.\"\n",
        "print(len(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4542\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCQJrKc5RCRB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def summarize(text):\n",
        "  inputs = tokenizer.encode(text, add_special_tokens=False, return_tensors=\"pt\")\n",
        "  outputs = model.generate(inputs, max_length=250, do_sample=True, top_p=0.95, top_k=60)\n",
        "  generated = tokenizer.decode(outputs[0])\n",
        "  return generated"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XM_yoBTix4Te",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "6750b965-247f-435a-d774-9182a1c451b7"
      },
      "source": [
        "sum = summarize(text)\n",
        "print(sum)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Four overnight camps in Maine successfully identified and isolated three Covid-19 positive people with no symptoms, preventing transmission to more than 1,000 other campers and staff this summer, according to a report published by the Centers for Disease Control and Prevention. Four overnight camps in Maine successfully identified and isolated three Covid-19 positive people with no symptoms, preventing transmission to more than 1,000 other campers and staff this summer, according to a report published by the Centers for Disease Control and Prevention.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGMJrvODyPNV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "42b44820-5bf4-4014-be60-57b26dce9368"
      },
      "source": [
        "print(sum)\n",
        "print(len(sum))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Four overnight camps in Maine successfully identified and isolated three Covid-19 positive people with no symptoms, preventing transmission to more than 1,000 other campers and staff this summer, according to a report published by the Centers for Disease Control and Prevention. Four overnight camps in Maine successfully identified and isolated three Covid-19 positive people with no symptoms, preventing transmission to more than 1,000 other campers and staff this summer, according to a report published by the Centers for Disease Control and Prevention.\n",
            "557\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSh3tYTr4RSG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}